---
title: "Panel Data Using R - A Simple Example"
toc: true
number-sections: false
format:
  html: 
    theme: cerulean
    fontsize: 1.1em
    backgroundcolor: '#ffffff'
    fontcolor: '#000000'
    linestretch: 1.7
    embed-resources: true
    fig-height: 5
    fig-width: 7.5
    code-fold: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
#| label: load-pkgs
#| code-summary: "Packages"
#| echo: false
#| message: false
#| warning: false

library(plm)
library(knitr)
library(broom)
library(tidyverse)
library(readxl)
library(readr)
library(stargazer)
library(lmtest)
library(gplots)
library(wooldridge)
library(scales)     
library(viridis)
library(formattable)
library(plotly)
```

## Introduction to Panel Data

Panel data, also known as longitudinal data, combines time series and cross-sectional data, providing repeated observations of multiple individuals, firms, or other units over time.  This structure offers several advantages in research, particularly in addressing questions that pure cross-sectional or time series data cannot adequately answer.  The significance of panel data lies in its ability to control for unobserved heterogeneity, which are individual-specific effects that are constant over time but may be correlated with the explanatory variables.  By accounting for these unobserved factors, panel data analysis can reduce omitted variable bias and lead to more accurate and robust estimates of the relationships between variables.

Furthermore, panel data allows researchers to study dynamic processes, such as how changes in one variable affect another over time, and to examine the effects of policy changes or interventions.  This makes panel data a valuable tool in a wide range of fields,

For example, we want to understand the factor affecting the rent across multiple cities for multiple time periods, then this type of data would be ideal candidate for panel data analysis. Panel data with missing values are called 'unbalanced Panel' whereas panel data with no missing values are called 'Balanced Panel'. 

Panel data analysis allows us to study individual heterogeneity and allows us control for observable variables of variables that change over time.

In this article, we will be using "Rental data". The dataset come from the publication **Introductory Econometrics: A Modern Approach, 7e** by Jeffrey M. Wooldridge, which is available as a dataset in R. The wooldridge package allows researchers to easily load any data set from the text. The package contains full documentation for each set and all data have been compressed. To load, just install the package, load it, and call the data you wish to work with. It is always recommended that one reads the supporting documentation for data sets of interest.

The objective of this exercise, is to understand the features that impact our dependent variable, (rent). Specifically, we are interested in understanding if one of the features, (student population) is a significant factor for rent.

## Data Exploration and Preparation

We load the data and do some initial exploration to understand the characteristics of the data.

```{r, warning=FALSE}
RENTAL<-read.csv("rentaldata.csv")
head(RENTAL)
```

You will observe that there are several variables in the  RENTAL dataset. The data set is panel data as it contains a combination of cross-sectional and time data.

It contains a city for cross section and year for time series. So, lets use these features as index and create a new dataset called rental_p. In this article, we will be focused on few selected columns to understand their relationship with dependent variables, so lets just keep the columns we need and drop the rest.

```{r}
rental_p<- pdata.frame(RENTAL, index = c("city", "year"))
keeps<- c("lrent", "y90", "lpop", "lavginc", "pctstu")
rental_view<- rental_p[keeps]
head(rental_view)
```

This dataset has rent related information of 64 different cities across 2 different years. The years have been labelled as either 80 or 90.

lrent refers to log of rent, which is our dependent variable, y90 refers to if the year is 90 or 80. Its 1 if year is 90 if not 0. Similarly, lavginc refers to log of average income, pctstu refers to percent of
population students.

```{r}
summary(rental_view)
```

## Modelling

In this article, I will explore three models, namely the OLS, the Fixed Effect and the Random Effects Model. Each model has its own purpose and is suited for a different type of panel data.

### OLS Model

OLS or the ordinary least square model. It is also termed as the pooled models. This is a Linear Regression model, which ignores the panel structure. It is efficient when the error terms in the model are homoscedastic and not autocorrelated.

```{r}
ols<- lm(lrent ~ y90+lpop+lavginc+pctstu, data= RENTAL) 
summary(ols)
```

Since the OLS ignores panel structure, we used the original rental data. The model shows that apart from the log of population, all other variables are significant for this model, and the model itself is significant.

### Pooled OLS MODEL using plm package

The 'plm' package is especially used for panel data analysis. Let's index the data for this model as we are using the origal dataset. The results below shows that both models produce same results.

```{r}
pooled<- plm(lrent~y90+lpop+lavginc+pctstu, 
             data= RENTAL,
             model = 'pooling', 
             index = c("city", "year"))
summary(pooled)
```

To tidy up the results we use the `stargazer` package, The `Stargazer` package is a great way to create tables to neatly represent your regression outputs. The package gives options to output tables in multiple formats: .txt, LaTex code, and as .html.

```{r, warning=FALSE}
stargazer(pooled, 
          type = "text", 
          digits = 2)
```

### Diagnostics - Homoscedasticity Assumption (OLS Model)

Heteroscedasticity assumes variability in the observations of the dependent variable. It is just the opposite of homoscedasticity. First, we will fetch the residuals and fitted values from our OLS model. Let's
take percentage of students against the residuals to have a look at the relationship of the model.

```{r}
res<- residuals(ols)
yhat<- fitted(ols)

plot(RENTAL$pctstu, res, 
     xlab="%students", 
     ylab= "Residuals")

plot(yhat, res, 
     xlab="Fitted values", 
     ylab= "Residuals")
```

We can see the spread of the data points which ensures the heteroscedastic nature as clustered patterns are visible. It just translates to the variability of the dependent variable which is the log of the rent value for our dataset. Therefore, we can conclude that OLS
is not really the best model to analyze our dataset.

## The Fixed Effect Model (FE)

The Fixed Effect model assumes variations within a cross-section which could be due to the inherent characteristics of that entity, which is "city" in this example. We control for the time invariant characteristics and study the net effect of the predictors in the outcome or rent variable for this dataset. The difference between the fixed effects and the OLS model is changing this model parameter from 'pooling' to 'within.'

```{r}
fe<- plm(lrent~y90+lpop+lavginc+pctstu, 
         data= rental_p,
         model = 'within')
summary(fe)
```

The model outcome is significant, but the log of the population remains insignificant for this relation, as it was in the OLS model. If we want to check the fixed effects for each city, we can use the fixef function to pull the values. So, to summarize, we have the fixed effects model with two out of four independent variables significant at 0.001 level and one more at 0.01 level or 1% level. Now we have to test whether fixed effects are better than OLS for this case.

```{r}
fixef(fe)
```

### Tests to Determine Superiority of FE Model over OLS

Here, the null hypothesis is OLS is better than FE model. If it is rejected at alpha of 0.05, we can proceed with the Fixed Effects model.

```{r}
pFtest(fe, ols)
```

We can see the p-value is much smaller than 5% level. Therefore, we can reject the null hypothesis and proceed with fixed effects model. Using `stargazer`, we tidy up the results

```{r, warning=FALSE}
stargazer(fe, 
          type= "text", 
          digits = 2)
```

Now, let's look into the random effects model and check whether that fits our data better.

## Random Effects Model

The random effects model includes the possibility of between entity variations. It also assumes that this variation is random in nature or they are uncorrelated with variables under study.

```{r}
re<- plm(lrent~y90+lpop+lavginc+pctstu, 
         data= rental_p,
         model = 'random')
summary(re)
```

We can see that the intercept is not significant. Again, we reproduce results in a tidy format.

```{r, warning=FALSE}
stargazer(re, 
          type= "text", 
          digits = 2)
```


As the overall model is significant, we proceed to the diagnostic tests to determine how the Random Effects model fares over the other models. We have already concluded that fixed effects model is better than the OLS model. So now we will compare between fixed effects and random effects model. We will use a very popular diagnostic test for this purpose called Hausman Test. The null hypothesis of this test states that random effects is preferred over fixed effects.

```{r}
phtest(fe, re)
```

The p-value here is less than 0.05 Therefore, we can reject the null hypothesis and conclude that the fixed effects model is more suited to rental data.

## Compare Results of Models

To do this, we use the `stargazer` package again. 

```{r}
stargazer(pooled, fe, re,
          type = "text",
          digits = 2,
          header = FALSE,
          title= "Regression Results",
          covariate.labels = c("Year1990", "log(population)", "Avg Income", "Student Pop"),
          dep.var.caption  = "Dependent Variable",
          column.labels   = c("Pooled", "FE","RE"),
          column.separate = c(1, 1, 1))
```

## Analysing the best model

Now we know that fixed effect model is the best for the data, we now analyze the relationship between independent and dependent variables from that model.

```{r}
summary(fe)
```

Our initial question we wanted to answer was if the student population has any impact on a city's rent. This model shows that the percentage of student population is significant at 1% level. In comparison, the other variables except population are significant at 0.1% level. The coefficients of the independent variables indicate how much the dependent variable changes over time on average per city, when the independent variables increase by one unit. The independent variables here can explain the dependent variable satisfactory. And this can be further validated by adjusted R-Squared of 95%

## Additional Diagnostics Tests for Panel Data

Here, I will show you few more diagnostic tests that you may require to analyze other datasets.

### Diagnostics - Existence of Panel Effects in data

The hypothesis of this test states that OLS is a better model and the alternative hypothesis suggests random effects to be a better model. Let us conduct the test on our pooled model.

```{r}
plmtest(pooled, type=c("bp"))
```

Since the value is much less than 0.05 level, we can reject the null hypothesis and conclude that panel effects indeed exist in the data.

The next two tests are specifically used for macro panels with longer time series. Therefore, it is not suitable for data like this one, which has only two time periods. But for illustration purposes we also include how the diagnostic tests can be used for research and analysis.

### Diagnostics: Cross-sectional dependence

Here we will use two test one is called Breusch-Pagan LM test and the other one is called Pesaran CD test. The null hypothesis for both test is that there is no cross-sectional dependence. If the p-value is less than 0.05 we reject the null and conclude that there is cross-sectional dependence.

```{r}
pcdtest(fe, test=c("lm"))
pcdtest(fe, test=c("cd"))
```

As i mentioned earlier, due to lack of time series, this test is not suitable to infer anything about the cross-sectional dependence for this data.

### Diagnostics: Test for serial correlation

The null hypothesis for this test is that there is no serial correlation. We reject the null if the p-value is less than 0.05 and conclude that serial correlation exists.

```{r}
pbgtest(fe)
```

So following the earlier reasons as above, this test is not suitable for the rental dataset.

### Diagnostics: Test for Heteroscedasticity

Since, I wanted to illustrate the effect of heteroscedasticity graphically, I did not include this test earlier along with the pooled model.

```{r}
bptest(lrent~y90+lpop+lavginc+pctstu+factor(city), 
       data= rental_p, 
       studentize = F)

```

The null hypothesis assumes homoscedasticity in the data. However, the result has a p-value less than 0.05 which allows us to reject the null and it conforms the initial visual analysis that heteroscedasticity exists in this
data set.

## References

1. Hlavac, Marek (2022). stargazer: Well-Formatted Regression and Summary Statistics Tables.
